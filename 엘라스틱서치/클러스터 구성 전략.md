
엘라스틱서치를 실제 서비스 환경에 도입해 사용하기 위한 주요 설정과 클러스터 구성 전략에 대해 정리한다.  
노드 역할을 어떻게 지정해야 하는지, 어떤 조합으로 클러스터를 구성해야 하는지 학습할 수 있다.  
그리고 데이터의 기밀성, 무결성을 지키고 인증, 권한 분리를 할 수 있도록 보안 기능을 활성화하는 방법도 학습할 수 있다.  
  
# 노드의 역할

엘라스틱서치의 노드에는 **역할** 이라는 개념이 있으며, 클러스터 구성을 위해 반드시 노드에 역할을 지정해야 한다.  
지정된 역할에 따라 노드가 클러스터 내에서 어떤 작업을 담당할지 정해진다.  
**각 노드는 여러 역할을 수행할 수도 있다.**  
  
1. **마스터 후보(master-eligible) 노드**
   - 노드의 역할에 master를 지정하면 해당 노드는 마스터 후보 노드가 된다.
   - 마스터 후보 노드 중에서 선거를 통해 마스터 노드가 선출된다.
   - 마스터 노드는 인덱스 생성이나 삭제, 어떤 샤드를 어느 노드에 할당할 것인지 등 **클러스터를 관리하는 역할을 수행한다.**
2. **데이터 노드**
   - 실제 데이터를 들고 있는 노드다.
   - CRUD, 검색, 집계와 같이 데이터와 관련된 작업을 수행한다.
3. **인제스트(ingest) 노드**
   - 데이터가 색인되기 전에 전처리를 수행하는 **[인제스트 파이프라인](https://www.elastic.co/guide/en/elasticsearch/reference/current/ingest.html)** 을 수행하는 노드다.
4. **조정 노드**
   - 클라이언트의 요청을 받아서 다른 노드에 요청을 분배하고 클라이언트에게 최종 응답을 돌려주는 노드를 조정 노드라고 한다.
   - **기본적으로 모든 노드가 조정 역할을 수행한다.**
   - 마스터나 데이터 등 주요 역할을 수행하지 않고 조정 역할만 수행하는 노드는 **조정 전용 노드** 라고 부른다.
5. **원격 클러스터 클라이언트 노드**
   - 다른 엘라스틱서치 클러스터에 클라이언트로 붙을 수 있는 노드다
   - 노드 역할에 `remote_cluster_client`를 추가해 지정하며 키바나의 스택 모니터링 기능을 활용해서 모니터링 전용 클러스터를 구축한 뒤 얼럿 메시지를 보내도록 구성하거나 유료 기능인 클러스터간 검색 기능 등을 활용할 때 사용된다.
7. **데이터 티어 노드**
   - 노드를 용도 및 성능별로 `hot-warm-cold-frozen` 티어로 구분해 저장하는 데이터 티어 구조 채택 시 사용하는 역할이다.

# 설정 정보

```yml
# 노드 역할 지정
# master, data, ingest 등을 0개 이상 조합해 지정한다.
# []와 같이 내용을 비워 두면 조정 전용 노드가 된다.
node.roles: [ master, data ]

cluster.name: test-es
node.name: test-es-node01

# HTTP 통신을 위해 사용하는 포트를 지정하며, 기본값은 9200-9300이다.
# 범위를 지정하면 범위 내의 선점되지 않은 포트 중 가장 앞쪽 포트를 사용한다.
http.port: 9200

# transport 통신을 위해 사용하는 포트를 지정하며, 기본값은 9300-9400이다.
# transport 프로토콜은 엘라스틱서치의 노드 사이의 내부 통신에 사용되는 프로토콜이다.
# 그렇다고 노드 사이의 모든 통신들이 transport를 사용하는 것은 아니며, 일부 HTTP 통신을 하기도 한다.
transport.port: 9300-9400

# 데이터 디렉터리로 pata.data처럼 여러 경로를 지정할 수 있다.
# 주로 여러 디스크를 골고루 사용하기 위해 여러 경로를 지정하는 경우가 많다.
# 하지만 여러 경로를 지정하는 설정은 7.13 버전 부터 지원 중단 선언되었다.
path.data1: /Users/jeonghyeonjun/Desktop/elasticsearch-data/data1
path.data2: /Users/jeonghyeonjun/Desktop/elasticsearch-data/data2
path.logs: /Users/jeonghyeonjun/Desktop/elasticsearch-data/logs

network.host: 10.0.0.1
# 엘라스틱서치에 바인딩할 네트워크 주소를 지정
network.bind_host: 0.0.0.0

# network.publish_host는 클러스터의 다른 노드에게 자신을 알릴 때 쓰는 주소를 지정한다.

# 마스터 노드로 동작할 수 있는 노드 목록을 지정한다.
discovery.seed_hosts: ["10.0.0.1", "10.0.0.2", "some-host-name.net"]

# 클러스터를 처음 기동할 때 첫 마스터 선거를 수행할 후보 노드 목록을 지정한다.
cluster.initial_master_nodes: ["test-es-node01", "test-es-node02", "test-es-node03"]
xpack.security.enabled: false
```

## 힙 크기

첫 번째 대원칙은 **최소한 시스템 메모리의 절반 이하로 지정해야 한다는 것이다.**  
루씬이 커널 시스템 캐시를 많이 활용하기 때문에 시스템 메모리의 절반은 운영체제가 캐시로 쓰도록 놔두는 것이 좋다.  
  
두 번째 대원칙은 **힙 크기를 32GB 이상 지정하지 않아야 한다는 것이다.**  
512GB 이상의 고용량 메모리를 갖춘 서버를 사용하더라도 힙 크기는 32GB 아래로 설정하라고 가이드 한다.  
JVM이 힙 영역에 생성된 객체에 접근하기 위한 포인터를 **Ordinary Object Pointer** 라고 하며 이 OOP는 메모리 주소를 직접 가리킨다.  
  
32비트 환경에서는 포인터 1개를 32비트로, 64비트 환경에서는 포인터 1개를 64비트로 표현한다.  
32비트 환경에서는 4GB 까지 힙 영역을 사용하기 때문에 4GB를 넘어서는 힙 영역을 사용해야 한다면 32비트 OOP로는 불가능하고 64비트 OOP를 사용해야 한다.  
  
하지만 32GB 이내의 힙 영역에만 접근한다면 **Compressed OOP** 라는 기능을 적용해 포인터를 32비트로 유지할 수 있다.  
자바는 기본적으로 힙 영역에 저장하는 객체를 **8바이트 단위로 정렬해 할당** 하기 때문에 객체 간의 주소는 항상 8바이트의 배수만큼 차이가 나기 때문에 적용 가능한 최적화 방법이다.  
따라서 포인터가 메모리의 주소를 직접 가리키도록 하지 않고 객체의 상대적인 위치 차이를 나타내도록 하면 포인터의 1비트가 1바이트 단위의 메모리 주소가 아니라 8바이트 단위의 메모리 주소를 가리키도록 할 수 있다.  
  
## 스와핑

스와핑은 성능과 노드에 큰 영향을 끼치고, 밀리세컨드 단위로 끝나야 할 GC를 분 단위 시간까지 걸리게 만들기 때문에 엘라스틱서치는 스와핑을 사용하지 않도록 강력히 권고한다.  
공식 문서에서는 스와핑을 켜느니 차라리 운영체제가 노드를 kill 시키도록 놔두는 것이 낫다고 가이드 한다.  
  
스와핑을 끄는 방법에 대한 설명은 [공식 문서](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html#setup-configuration-memory)를 참고하자.  

# 클러스터 구성 전략

```json
GET _cluster/settings

{
  "persistent": {},
  "transient": {}
}
```

`persistent`는 클러스터를 풀 리스타트로 전체 재시작해도 유지되며 `transient`는 클러스터를 전체 재시작하면 내용이 사라진다.  
엘라스틱서치의 설정 적용 우선순위는 `transient`, `persistent`, `config/elasticsearch.yml` 순이다.  
추가로 [cat API](https://www.elastic.co/guide/en/elasticsearch/reference/current/cat.html)를 통해 상태를 조회할 수도 있다.  
  
1. **마스터 후보 노드와 데이터 노드를 분리**
   - 상대적으로 데이터 노드가 죽을 확률이 높다.
   - 분리된 상황에서 데이터 노드만 죽으면 마스터 노드가 주 샤드가 없어진 샤드의 복제본을 새로운 주 샤드로 변경시키고, 복제본 개수를 새로 채우는 과정을 수행하며 자연스럽게 클러스터가 문제 상황에서 복구된다.
   - 마스터 후보 노드와 데이터 노드를 분리하지 않으면 마스터 재선출 과정과 샤드 복구 과정이 같이 수행되기 때문에 장애 상황에 대응하기에는 리스크가 너무 크다.
   - **마스터 후보 노드는 디스크나 메모리를 많이 필요로 하지 않기 때문에 데이터 노드보다 상대적으로 성능이 많이 낮은 서버를 사용해도 괜찮다.**
2. **마스터 후보 노드와 투표 구성원**
   - 마스터 노드는 클러스터를 관리하는 중요한 역할을 수행한다.
   - 클러스터가 운영되는 동안 항상 마스터 노드가 선출되어 있어야 하며, 이 마스터 노드를 선출하는 집단이 **투표 구성원** 이다.
   - **투표 구성원** : 마스터 후보 노드 중 일부 혹은 전체이며, 마스터 선출이나 클러스터 상태 저장 등의 의사결정에 참여하는 노드의 모임
   - 마스터 후보 노드가 투표 구성원에 참여하거나 제거될 때에는 조정 동작이 발생하며 어느 정도 시간이 소요된다.
   - 마스터 후보 노드의 수를 줄이고 싶다면 한 대씩 클러스터에서 제거하여야 한다.
   - 마스터 후보 노드는 홀수대를 준비하는 것이 비용 대비 효용성이 좋다.
   - [split brain 문제](https://esbook.kimjmin.net/03-cluster/3.3-master-and-data-nodes#split-brain)
3. **서버 자원이 많지 않은 경우**
   - 적어도 마스터 후보 역할을 하는 노드 3대, 데이터 노드 3대가 확보돼야 기본적인 고가용성이 제공되기 때문에 이 시점부터 클러스터를 구성하는 의미가 있다.  
   - 최소한 마스터 후보 역할은 꼭 3대를 지정해야 한다.
4. **서버 자원이 굉장히 많이 필요한 경우**
   - 마스터 후보 노드도 높은 사양의 서버를 할당하거나 노드의 수가 더 늘어나야 한다면 서비스 구조의 복잡도와 관리 비용을 더 높이더라도 강제로 클러스터를 여러 개로 찢어야 한다.
   - 같은 데이터의 싱크를 맞추고 있는 클러스터를 여러 개 구성한 다음 앞단에서 **로드 밸런싱을 하는 전략** , **클러스터별로 데이터를 샤딩하는 전략** , **클러스터 간 검색(cross-cluster search) 도입** 등도 고려해야 한다.
6. **조정 전용 노드**
   - 안정적인 클러스터 운영을 위해서는 조정 전용 노드를 두는 것이 좋다.
   - 각 데이터 노드에서 작업한 결과물을 모아서 돌려주는 작업은 생각보다 큰 부하를 줄 수 있다.
   - 데이터를 들고 있지 않기 때문에 샤드 복구와 관련된 걱정도 없으며 부담없이 프로세스를 Kill 시킬 수 있다.
   - 조정 전용 노드의 읽기 작업과 쓰기 작업을 분리시킬수도 있다.
7. **한 서버에 여러 프로세스 띄우기**
   - 마스터 후보 역할을 하는 프로세스는 다중 프로세스 대상으로 고려하지 말아야 하고 여러 프로세스를 띄우는 대상은 데이터 노드여야 한다.
   
  
> **홀수대의 마스터 후보 노드를 준비하는 것이 좋은 이유**  
> 엘라스틱서치가 투표 구성원을 홀수로 유지하기 위해 투표 구성원에서 마스터 후보 노드를 하나 빼 두기 때문이다.  
> 이 한 대는 투표 구성원에 참여하지 않는다. 마스터 후보 노드가 2k + 1대일 때보다 2k + 2대일 때가 약간 더 안정성이 높기는 하다.  
> 투표 구성원에 포함돼 있던 노드 하나가 죽으면 투표 구성원에는 빠져 있던 나머지 노드가 대신 투표 구성원으로 들어온다.  
> 천천히 한 대씩 순차적으로 죽는 경우라면 1대의 실패를 더 견딜 수 있다.  
> 다만 여러 대가 투표 구성원 조정 전에 동시에 죽는 상황이라면 마스터 후보가 2k + 1대일 때나 2k + 2대일 때나 동일하게 k대의 동시 실패만을 견디기 때문에 홀수대를 준비하는 것이 비용대비 효용성이 좋다.

# 보안 기능 적용

기본적으로 특별한 설정을 하지 않으면 클라이언트와 엘라스틱서치 클러스터, 엘라스틱서치 노드 간의 통신은 암호화되지 않는다.  
외부 인터넷 환경에 노출되어 있다면 정보의 기밀성과 무결성을 보장할 수 없다.  

## TLS 부트스트랩 체크

엘라스틱서치 클러스터는 기동 과정에서 노드 간 transport 통신에 TLS를 적용하지 않았다면 기동을 거부한다.  
`discovery.type`을 `single-node`로 지정해서 개발 모드로 띄웠거나 아예 `xpack.security.enabled`를 `false`로 지정했으면 TLS 부트스트랩 체크를 건너뛴다.  
  
문제는 보안 설정을 하기 위한 몇몇 중요한 작업이 클러스터를 일단 기동한 상태에서 진행해야 하지만 자동 보안 설정을 사용하려면 아무런 보안 설정을 하지 않은 상태에서 일단 클러스터를 최초 기동해야 하는데, 노드 간 TLS 통신이 적용되지 않았기 때문에 TLS 부트스트랩에 걸려 최초 기동이 되지 않고, 클러스터가 가동되지 않는다.  
TLS 부트스트랩을 우회하며 여러 번에 걸쳐 작업을 진행해야 한다.  
그리고 노드 간 TLS 통신을 적용하기 전까지는 TLS 부트스트랩 체크에 걸려 기본 인증 적용도 진행할 수 없다.  
  
구체적으로 어떻게 진행하는지 알아보자.  

## 클러스터 최초 기동 시 자동 보안 설정 이용

클러스터 최초 기동 시 다른 보안 설정이 지정되지 않았을 때 수행된다.

- 전용 CA를 생성하고 그 CA로 서명된 인증서를 자동 발급한다.
- 노드 간 transport 레이어와 REST API에 사용하는 HTTP 레이어에 TLS 통신을 적용한다.
- 최고 관리자 권한을 가진 계정의 비밀번호를 초기화한다.
- 이후 클러스터에 합류하는 노드에도 인증서를 복사하며 자동으로 설정을 넣는다.  
- 키바나에도 필요한 CA 파일을 복사하고 설정을 자동으로 기입한다.

# 샤드의 크기와 개수 조정

인덱스의 샤드 개수는 한 번 지정하면 `reindex`의 특별한 작업을 수행하지 않는 한 변경할 수 없다.  
그런데 샤드 개수를 어떻게 지정하느냐에 따라 엘라스틱서치 클러스터 전체의 성능이 크게 달라진다.  
  
클러스터에 샤드 숫자가 너무 많아지면 샤드 하나당 루씬 인덱스가 하나씩 더 뜨며 힙을 차지하기 때문에 클러스터 성능이 눈에 띄게 떨어진다.  
주 샤드를 하나 더 띄울때 마다 복제본 샤드도 늘어나는 것을 고려하면 무작정 `number_of_shards`를 늘릴 수는 없다.  
그렇다고 샤드 숫자를 적게 지정하면 샤드 하나당 크기는 커진다.  
  
중요한 원칙은 **샤드 하나의 크기를 일정 기준 이하로 유지해야 한다는 것** 이다.  
전체 샤드의 수를 체크하는 것은 그 다음이다.  
샤드의 크기를 기준 크기 이하로 유지하는 선에서 전체 샤드 개수를 최대한 줄이는 방향으로 접근해야 한다.  
  
이와 관련해서 엘라스틱 블로그에 [가이드](https://www.elastic.co/kr/blog/how-many-shards-should-i-have-in-my-elasticsearch-cluster)가 있다.  
여기서는 샤드 하나 당 `20 ~ 40GB`크기가 적절한다고 소개한다.  
하지만 실제로는 **인덱스에 어떤 성격의 데이터가 얼마나 있는지에 따라 양상이 매우 다르다.**  
여러 통제된 조건에서 테스트를 해보고 결정하는 것이 좋다.  

```
# 샤드의 크기를 확인
GET _cat/shards?v&s=store:desc

# 샤드 개수 확인
GET _cat/health?v
GET _cat/allocation?v
```

<h3>모든 노드가 충분히 일을 하고 있는지</h3>

샤드는 이름 자체가 의미하듯 샤딩, 즉 분산처리를 위해 생긴 개념이다.  
전체 샤드 개수를 줄이는 것만 생각하다 보면 분산처리의 강점을 충분히 살리지 못할 수 있다.  
**특정 인덱스의 주 샤드와 복제본 샤드가 모든 노드에 고르게 퍼지도록 설정하는 것도 중요한 요소다.**  
노드 대수가 `n`대라면 `number_of_shards`를 `n의 배수`로 지정해 모든 노드가 작업을 고르게 분산받도록 설정하는 방법 등이 사용된다.  
  
서비스상 빈번하게 호출되지 않는 인덱스나 크기가 너무 작은 인덱스의 샤드를 모든 노드에 배치시킬 필요는 없다.  

# 롤링 리스타트

동적으로 변경할 수 없는 설정의 적용, 플러그인 설치나 삭제의 적용, 엘라스틱서치의 버전 업그레이드 등 다양한 상황에서 롤링 리스타트가 필요하다.  
그리고 무엇보다 **장애 상황에서 문제를 일으키고 있는 노드를 재기동 하기 위해 많이 수행된다.**  

1. **샤드 할당 비활성화**
   - 노드를 재기동하기 위해 엘라스틱서치 프로세스를 종료시키면 클러스터 구성에서 노드가 빠진다.
   - 빠진 노드가 데이터 노드라면 주 샤드를 새로 지정하고 줄어든 복제본 개수를 맞추기 위해 복제본 샤드를 새로 할당해 생성하는 작업이 수행된다.
   - 재기동한 노드가 클러스터에 합류하면 자신이 들고 있던 샤드를 복제본 샤드로 다시 띄우기 때문에 **롤링 리스타트 과정에서는 복제본 샤드를 새로 생성하는 작업이 필요없다.**
   - 하지만 재기동한 노드가 클러스터 입장에서는 노드가 재기동을 위해 빠졌는지 장애로 인해 빠져는지 구분할 수 없다. 그저 노드가 빠지면 일정 시간 대기한 뒤 복제본 샤드 할당 작업을 수행할 뿐이다.
   - `cluster.routing.allocation.enable` 설정을 동적으로 변경하며 샤드 할당 전략을 수정하고 롤링 리스타트를 진행해야 한다.
2. **flush 수행**
   - translog를 비우고 데이터를 디스크에 안전하게 기록한다.
   - 루씬 인덱스에 반영되지 않고 아직 translog에 남아 있는 내용은 노드 재기동 시 샤드 복구 과정에서 처리되기 때문에 이 작업은 반드시 필요한 작업은 아니다.
   - `POST _flush`
   - 이전에는 `sync_id`라는 마커를 샤드에 지정하여 색인이 새로 들어왔는지 비교하는 방법을 사용했지만 최신 버전에서는 **샤드 이력 보존** 이라는 메커니즘을 사용한다.
3. **노드 재기동**
   - 샤드 할당을 비활성화하고 flush를 무사히 완료했으면 노드를 재기동한다.
   - 프로세스를 kill하고 새로 띄우면 된다.
   - 이후 노드가 완전히 기동하고 클러스터에 합류할 때까지 대기한다.
   - `GET _cat/nodes`로 재기동한 노드가 클러스터에 잘 붙었는지 확인한다.
4. **샤드 할당 활성화**
   - 노드 재기동에 성공했으면 샤드 할당을 다시 활성화한다.
   - `cluster.routing.allocation.enable`를 `all`로 다시 활성화한다.
5. **green 상태 까지 대기**
   - `GET _cat/health?v`로 상태가 green이 될때까지 대기한다.

# 스냅샷과 복구

데이터 백업은 실제 서비스를 운영할 때 빼놓을 수 없는 중요한 이슈다.  
**스냅샷** 은 동작 중인 엘라스틱서치 클러스터의 인덱스를 백업하는 방법이다. 인덱스를 원하는 다양한 스토리지에 스냅샷으로 찍어두면 필요할 때 복구할 수 있다.  

<h3>스냅샷 저장소 등록과 설정</h3>

스냅샷 저장소로는 다양한 종류의 파일 시스템을 선택할 수 있다.  
기본적으로 지원되는 것은 nfs와 같은 공유 파일 시스템 타입이다.  

```json
PUT _snapshot/[저장소 이름]
{
  "type": "fs",
  "settings": {
    "location": "/my/nfs/mount/path"
  }
}

PUT _snapshot/[저장소 이름]/[스냅샷 이름]?wait_for_completion=false
{
    "indices" : ...
}

GET _snapshot/[저장소 이름]/[스냅샷 이름]
```

마운트 경로는 사전에 elasticsearch.yml에 `path.repo` 설정으로 등록해야 한다.

# 데이터 티어 구조

데이터 노드를 용도 및 성능 별로 `hot-warm-cold-frozen` 티어로 구분해 클러스터를 구성하는 방법이다.  
노드의 역할로 data를 지정하지 않고 `data_content`, `data_hot`, `data_warm`, `data_cold`, `data_frozen`을 지정해 클러스터를 구성한다.  
성능 차이가 많이 나는 장비를 가지고 클러스터를 구성해야 하거나 최근 데이터 위주로 사용하는 방식의 시계열 데이터를 운영할 때 채택하기 좋다.  
  
<h3>인덱스 생명 주기 관리</h3>

데이터 티어 구조에서 지정한 기간이 지나면 인덱스를 다음 페이즈로 전환시키고 이때 지정한 작업을 수행하도록 하는 기능이다.  
인덱스와 데이터 스트림을 매우 편하게 관리할 수 있게 해준다.  

# 서킷 브레이커

엘라스틱서치는 노드가 중단되는 사태를 방지하는 것이 더 중요하다고 판단하여 처음부터 과도한 요청은 거부하는 정책을 채택했다.  
서킷 브레이커는 **문제를 발생시킬 만한 무거운 작업의 수행을 사전 차단한다.**  
요청이 어느 정도의 메모리를 사용할지 예상하는 방법과 실제 현재 노드가 사용 중인 메모리를 체크하는 방법을 사용한다.  
  
서킷 브레이커 발동으로 요청이 거부됐다고 해서 엘라스틱서치가 이를 자동으로 재시도해 주지는 않는다. 전적으로 클라이언트가 책임져야 한다.

1. **필드 데이터 서킷브레이커**
   - fielddata가 메모리에 올라갈 때 얼마만큼의 메모리를 사용할지 예상한다.
   - 기본 값은 힙의 40%다.
   - `indices.breaker.fielddata.limit`으로 설정한다.
2. **요청 서킷 브레이커**
   - 요청 하나의 데이터 구조가 메모리를 과다하게 사용하는지 계산한다.
   - 기본 값은 힙의 60% 이며, `indices.breaker.request.limit`으로 설정한다.
3. **실행 중 요청 서킷 브레이커**
   - 노드에 transport나 HTTP를 통해 들어오는 모든 요청의 길이를 기반으로 메모리 사용량을 계산한다.
   - 텍스트 원본 길이만을 따지는 것이 아니라 요청 객체를 생성할 때 필요로 하는 메모리로 따진다.
   - 기본 값은 힙의 100% 이며, `network.breaker.inflight_requests.limit`으로 설정한다.
4. **부모 서킷 브레이커**
   - 전체 메모리의 실제 사용량을 기준으로 동작한다.
   - 또한 다른 자식 서킷 브레이커가 산정한 예상 메모리 사용량의 총합도 체크한다.
   - `indices.breaker.total.use_real_memory` 설정을 false로 변경하면 메모리의 실제 사용량은 체크하지 않으며 기본 값은 힙의 70%다. 이 값이 true인 경우 기본값은 힙의 95%이다.