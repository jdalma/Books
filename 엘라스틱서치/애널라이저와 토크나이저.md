# 애널라이저

애널라이저는 **0개 이상의 캐릭터 필터** , **1개의 토크나이저** , **0개 이상의 토큰 필터** 로 구성된다.  
동작도 위의 작성 순서대로 진행된다.  
  
1. 입력한 텍스트에 **캐릭터 필터** 를 적용하여 문자열을 변형 시킨 뒤
2. **토크나이저** 를 적용하여 여러 토큰으로 쪼갠다.
3. 쪼개진 토큰의 스트림에 **토큰 필터** 를 적용해서 토큰에 특정한 변형을 가한 결과가 최족정으로 분석 완료된 텀이다.

analyze API를 통하여 테스트할 수 있다.  

```json
POST _analyze
{
  "analyzer": "standard",
  "text": "Hello, HELLO, World!"
}

{
  "tokens": [
    {
      "token": "hello",
      "start_offset": 0,
      "end_offset": 5,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "hello",
      "start_offset": 7,
      "end_offset": 12,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "world",
      "start_offset": 14,
      "end_offset": 19,
      "type": "<ALPHANUM>",
      "position": 2
    }
  ]
}
```

## 캐릭터 필터

**텍스트를 캐릭터의 스트림으로 받아서 특정한 문자를 추가, 변경, 삭제한다.**  
여러 캐릭터 필터가 지정됐다면 순서대로 수행된다.  
ES에는 아래와 같은 내장 빌트인 캐릭터 필터가 있다.

1. **HTML strip 캐릭터 필터** : 태그 요소 안쪽의 데이터를 꺼낸다. `&apos;` 같은 HTML 엔티티도 디코딩한다.
2. **mapping 캐릭터 필터** : 치환할 대상이 되는 문자와 치환 문자를 맵 형태로 선언한다.
3. **pattern replace 캐릭터 필터** : 정규 표현식을 이용해서 문자를 치환한다.

```json
POST _analyze
{
  "char_filter": ["html_strip"],
  "text": "<p>I&apos;m so <b>happy</b>!</p>"
}

{
  "tokens": [
    {
      "token": """
I'm so happy!
""",
      "start_offset": 0,
      "end_offset": 32,
      "type": "word",
      "position": 0
    }
  ]
}
```

## 토크나이저

토크나이저는 캐릭터 스트림을 받아서 여러 토큰으로 쪼개어 토큰 스트림을 만든다.  
애널라이저에는 한 개의 토크나이저만 지정할 수 있다.  

1. **standard 토크나이저** : 텍스트를 단어 단위로 나누며, 대부분의 문장 부호가 사라진다. 필드 매핑에 특정 애널라이저를 지정하지 않으면 기본 값으로 해당 토크나이저가 적용된다.
2. **keyword 토크나이저** : 들어온 텍스트를 쪼개지 않고 그대로 내보낸다.
3. **ngram 토크나이저** : 텍스트를 `min_gram` 값 이상 `max_gram` 값 이하의 단위로 쪼갠다.
4. **letter 토크나이저** : 공백, 특수문자 등 언어의 글자로 분류되는 문자가 아닌 문자를 만났을 때 쪼갠다.
5. **whitespace 토크나이저** : 공백 문자를 만났을 때 쪼갠다.
6. **pattern 토크나이저** : 지정한 정규표현식을 단어의 구분자로 사용하여 쪼갠다.

```json
# keyword 토크나이저
POST _analyze
{
  "tokenizer": "keyword",
  "text": "Hello, HELLO! world!"
}

{
  "tokens": [
    {
      "token": "Hello, HELLO! world!",
      "start_offset": 0,
      "end_offset": 20,
      "type": "word",
      "position": 0
    }
  ]
}

# ngram 토크나이저
POST _analyze
{
  "tokenizer": {
    "type": "ngram",
    "min_gram": 3,
    "max_gram": 4
  },
  "text": "Hello, World!"
}

{
  "tokens": [
    {
      "token": "o, ",
      "start_offset": 4,
      "end_offset": 7,
      "type": "word",
      "position": 8
    },
    {
      "token": "o, W",
      "start_offset": 4,
      "end_offset": 8,
      "type": "word",
      "position": 9
    },
    {
      "token": "ld!",
      "start_offset": 10,
      "end_offset": 13,
      "type": "word",
      "position": 20
    },
    ...
}
```

ngram 토크나이저 결과로 활용 의미가 없는 토큰도 포함되기 때문에 **token_chars** 속성을 통해 토큰에 포함시킬 타입의 문자를 지정할 수도 있다.  

```json
POST _analyze
{
  "tokenizer": {
    "type": "ngram",
    "min_gram": 3,
    "max_gram": 4,
    "token_chars": ["letter"]
  },
  "text": "Hello, World!"
}
```

**ngram 토크나이저는 엘라스틱서치에서 RDB의 `LIKE *검색어*`와 유사한 검색을 구현하고 싶을 때, 자동 완성 관련 서비스를 구현하고 싶을 때 등에 주로 활용한다.**  

## 토큰 필터

토큰 필터는 토큰 스트림을 받아서 토큰을 추가, 변경, 삭제한다.  

1. **lowercase / uppercase 토큰 필터**
2. **stop 토큰 필터** : 불용어를 지정하여 제거할 수 있다.
3. **synonym 토큰 필터** : 유의어 사전 파일을 지정하여 지정된 유의어를 치환한다.
4. **pattern_replace 토큰 필터** : 정규식을 사용하여 토큰의 내용을 치환한다.
5. **trim 토큰 필터**
6. **truncate 토큰 필터**

```json
POST _analyze
{
  "filter": ["lowercase"],
  "text": "Hello, World!"
}

{
  "tokens": [
    {
      "token": "hello, world!",
      "start_offset": 0,
      "end_offset": 13,
      "type": "word",
      "position": 0
    }
  ]
}
```